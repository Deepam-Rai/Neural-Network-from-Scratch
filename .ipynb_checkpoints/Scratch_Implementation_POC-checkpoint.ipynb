{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2f78816-5b26-4b5b-a6bd-60b7adc9d92d",
   "metadata": {},
   "source": [
    "# Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "832df410-e753-44dc-b73b-8f4ad14d1e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'weights': array([[0.37183975, 0.93632696],\n",
      "       [0.31424118, 0.19147913]]), 'bias': False}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Linear():\n",
    "  '''A Linear Layer'''\n",
    "  def __init__(self,in_features,out_features, bias=True, requires_grad=True):\n",
    "    self.in_features = in_features\n",
    "    self.out_features = out_features\n",
    "    self.requires_grad = requires_grad\n",
    "    \n",
    "    self.weights = np.random.rand(self.out_features,self.in_features)\n",
    "    if bias:\n",
    "      self.bias = np.random.rand(self.out_features)\n",
    "    else:\n",
    "      self.bias = False\n",
    "  \n",
    "  \n",
    "  def forward(self,input):\n",
    "    if not isinstance(input, np.ndarray):\n",
    "      raise TypeError(\"Input must be a numpy nd.array.\")\n",
    "    if self.weights.shape[1] != input.shape[0]:\n",
    "      raise ValueError(\"Cannot multiply matrix of dimension \",self.weights.shape,\" with input of dimension \",input.shape)\n",
    "    \n",
    "    if self.requires_grad:\n",
    "      self.input = input\n",
    "    \n",
    "    result = np.dot(self.weights, input)\n",
    "    if isinstance(self.bias, np.ndarray):\n",
    "      result += self.bias\n",
    "    return result\n",
    "  \n",
    "  \n",
    "  def deriv_wrt_weight(self):\n",
    "    if not hasattr(self,'input'):\n",
    "      raise RuntimeError('backward() called before forward()')\n",
    "    return self.input #previous layer h\n",
    "  \n",
    "  def deriv_wrt_bias(self):\n",
    "    return 1\n",
    "  \n",
    "  def deriv_wrt_input(self):\n",
    "    return self.weights\n",
    "  \n",
    "  def backward(self, front_layer_grads):\n",
    "    if self.requires_grad:\n",
    "      self.weight_grads = self.deriv_wrt_weight() * front_layer_grads #chain rule\n",
    "      self.bias_grads = (self.deriv_wrt_bias() * front_layer_grads).reshape(1,-1)\n",
    "    return np.dot(front_layer_grads.transpose(),self.deriv_wrt_input()) #chain rule for the back layer grads\n",
    "  \n",
    "  def get_weight_grads(self):\n",
    "    if not self.requires_grad:\n",
    "      raise RuntimeError('requires_grad set to False. Cannot calculate gradients.')\n",
    "    if not hasattr(self,'weight_grads'):\n",
    "      raise RuntimeError('get_weight_grads() called before backward()')\n",
    "    return self.weight_grads\n",
    "  \n",
    "  def get_bias_grads(self):\n",
    "    if not self.requires_grad:\n",
    "      raise RuntimeError('requires_grad set to False. Cannot calculate gradients.')\n",
    "    if not hasattr(self,'bias_grads'):\n",
    "      raise RuntimeError('get_bias_grads() called before backward()')\n",
    "    return self.bias_grads\n",
    "  \n",
    "  def get_grads(self):\n",
    "    return {'weights':self.get_weight_grads(), 'bias':self.get_bias_grads(), 'requires_grad':self.requires_grad}\n",
    "  \n",
    "  \n",
    "  def __call__(self,input):\n",
    "    #overloading to use module as a function\n",
    "    return self.forward(input)\n",
    "  \n",
    "  def __repr__(self):\n",
    "    return f'Linear(in_features={self.in_features}, out_features={self.out_features}, bias={True if isinstance(self.bias,np.ndarray) else False})'\n",
    "  \n",
    "  def parameters(self):\n",
    "    return {'weights':self.weights, 'bias':self.bias}\n",
    "\n",
    "\n",
    "L = Linear(2,2,bias=False)\n",
    "print(L.parameters())\n",
    "# L.forward(np.array([3,1]))\n",
    "# print(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f223c42c-8110-43e1-9aff-09c3095de51f",
   "metadata": {},
   "source": [
    "# Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "860ffd81-e5f1-4e3a-9ab9-dfd4dbd9b0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ActivationFunction():\n",
    "  def __init__(self, requires_grad=True):\n",
    "    '''requires_grad: Necessarily required to be True for backpropagation.'''\n",
    "    super(ActivationFunction,self).__init__()\n",
    "    self.__parameters = []\n",
    "    self.requires_grad = requires_grad\n",
    "  \n",
    "  def parameters(self):\n",
    "    return self.__parameters\n",
    "  \n",
    "  def __setattr__(self, name,value):\n",
    "    super().__setattr__(name,value)\n",
    "    if not callable(value) and name!='_ActivationFunction__parameters':\n",
    "      self.__parameters.append((name,value))\n",
    "  \n",
    "  def __call__(self,input):\n",
    "    if not isinstance(input, np.ndarray):\n",
    "      raise TypeError(\"Input must be of type numpy nd.array.\")\n",
    "    if not hasattr(self,'function'):\n",
    "      raise NameError(\"The function has not been defined for this activation funcion.\")\n",
    "    if self.requires_grad:\n",
    "      self.input = input\n",
    "    return np.array([self.function(e) for e in input])\n",
    "  \n",
    "  def backward(self,front_layer_grads):\n",
    "    if not self.requires_grad:\n",
    "      raise RuntimeError(\"requires_grad set to False. Cannot calculate gradients.\")\n",
    "    if not hasattr(self,'derivative_function'):\n",
    "      raise NameError(\"derivative_function not set for this activation function. Cannot calculate gradients.\")\n",
    "    return np.array([self.derivative_function(e) for e in self.input]).reshape(-1,1)*front_layer_grads.reshape(-1,1)\n",
    "    \n",
    "  def __repr__(self):\n",
    "    return self.__class__.__name__ + \"()\"\n",
    "\n",
    "act = ActivationFunction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31029f80-d51a-4ed6-879b-ac86f01ec7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ReLU(ActivationFunction):\n",
    "  def __init__(self,requires_grad=True):\n",
    "    super(ReLU, self).__init__(requires_grad)\n",
    "    self.function = lambda x: 0 if x<0 else x\n",
    "\n",
    "activation = ReLU()\n",
    "# activation(np.array([1,2,-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7688fd2a-ef50-4eea-9baa-c4795cb287e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.73105858, 0.88079708, 0.26894142])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Sigmoid(ActivationFunction):\n",
    "  def __init__(self, requires_grad=True):\n",
    "    super(Sigmoid,self).__init__(requires_grad)\n",
    "    self.function = lambda x: 1/(1+np.exp(-x))\n",
    "    self.derivative_function = lambda x: self.function(x)*(1-self.function(x))\n",
    "\n",
    "activation = Sigmoid()\n",
    "activation(np.array([1,2,-1]))\n",
    "# print(activation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39de90ec-42a8-4262-96bb-2e9338ccaee1",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38e19084-4e5a-47fd-93a9-151f9f40ba48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleNet(\n",
      "  (hidden1): Linear(in_features=2, out_features=2, bias=True)\n",
      "  (activation1): Sigmoid()\n",
      "  (output): Linear(in_features=2, out_features=2, bias=True)\n",
      "  (final_activation): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork():\n",
    "  def __init__(self):\n",
    "    # super(NeuralNetwork, self).__init__()\n",
    "    self.__layers = []\n",
    "  \n",
    "  def __setattr__(self, name,value):\n",
    "    if isinstance(value,Linear) or isinstance(value,ActivationFunction):\n",
    "      self.__layers.append((name,value))\n",
    "    super().__setattr__(name,value)\n",
    "  \n",
    "  def __repr__(self):\n",
    "    string = self.__class__.__name__ + \"(\\n\"\n",
    "    for name,layer in self.__layers:\n",
    "      string += \"  (\" + name +\"): \" + layer.__repr__() + \"\\n\"\n",
    "    string += \")\"\n",
    "    return string\n",
    "  \n",
    "  def parameters(self):\n",
    "    '''Doesnt return the references. To be used to just view the parameters.'''\n",
    "    dicts = {}\n",
    "    for name, layer in self.__layers:\n",
    "      if not isinstance(layer,ActivationFunction):\n",
    "        dicts[name] = layer.parameters()\n",
    "    return dicts\n",
    "  \n",
    "  def param_references(self):\n",
    "    '''To be used for updating the parameters.'''\n",
    "    return self.__layers\n",
    "  \n",
    "  def backward(self,loss_function_grads):\n",
    "    grads = loss_function_grads\n",
    "    for name,layer in self.__layers[::-1]:\n",
    "      grads = layer.backward(grads)\n",
    "  \n",
    "  def get_grads(self):\n",
    "    dicts={}\n",
    "    for name,layer in self.__layers:\n",
    "      if not isinstance(layer,ActivationFunction):\n",
    "        dicts[name]=layer.get_grads()\n",
    "    return dicts\n",
    "  \n",
    "  def forward(self,x):\n",
    "    return x\n",
    "\n",
    "  \n",
    "  \n",
    "\n",
    "class SimpleNet(NeuralNetwork):\n",
    "  def __init__(self):\n",
    "    super(SimpleNet,self).__init__()\n",
    "    \n",
    "    self.hidden1 = Linear(in_features=2,out_features=2)\n",
    "    self.activation1 = Sigmoid()\n",
    "    self.output = Linear(2,2)\n",
    "    self.final_activation = Sigmoid()\n",
    "  \n",
    "  def forward(self,x):\n",
    "    h = self.hidden1(x)\n",
    "    h = self.activation1(h)\n",
    "    h = self.output(h)\n",
    "    h = self.final_activation(h)\n",
    "    return h\n",
    "\n",
    "\n",
    "model = SimpleNet()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a2e1db-18ce-4eba-8f08-38d6bd89782d",
   "metadata": {},
   "source": [
    "# Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83dd90be-7509-4225-ab2d-2ded653d073c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden1 :\n",
      " {'weights': array([[0.15, 0.25],\n",
      "       [0.2 , 0.3 ]]), 'bias': array([0.35, 0.35])} :\n",
      "\n",
      "output :\n",
      " {'weights': array([[0.4 , 0.5 ],\n",
      "       [0.45, 0.55]]), 'bias': array([0.6, 0.6])} :\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# setting the manual weights; randomized by default\n",
    "\n",
    "model.hidden1.weights = np.array([ [0.15, 0.25],\n",
    "                                   [0.2, 0.3]])\n",
    "model.hidden1.bias = np.array([0.35,0.35])\n",
    "\n",
    "model.output.weights = np.array([ [0.4, 0.5],\n",
    "                                  [0.45, 0.55]])\n",
    "model.output.bias = np.array([0.6,0.6])\n",
    "\n",
    "for layer,weights in  model.parameters().items():\n",
    "  print(layer,\":\\n\",weights,\":\\n\")\n",
    "# d = model.parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3a96b52-b880-4bf8-97a2-28e024f20703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.75693192, 0.76771788])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([0.05, 0.1])\n",
    "ypred = model.forward(X)\n",
    "ypred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5dcc87-ba46-4090-bc39-003bb4caf71c",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4387f4f6-3e23-4dbe-a63a-8d585f57f838",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LossFunction():\n",
    "  def __init__(self, requires_grad=True):\n",
    "    '''requires_grad: Necessarily True for backpropagation.'''\n",
    "    self.__parameters = []\n",
    "    self.requires_grad = requires_grad\n",
    "    \n",
    "  def __setattr__(self, name,value):\n",
    "    if not callable(value) and name!='_LossFunction__parameters':\n",
    "      self.__parameters.append((name,value))\n",
    "    super().__setattr__(name,value)\n",
    "  \n",
    "  def parameters(self):\n",
    "    return self.__parameters\n",
    "  \n",
    "  def __call__(self,yhat,y):\n",
    "    if not isinstance(yhat, np.ndarray) or not isinstance(y, np.ndarray):\n",
    "      raise TypeError(\"yhat and y both needs to be of type Numpy nd.array.\")\n",
    "    if self.requires_grad:\n",
    "      self.yhat = yhat\n",
    "      self.y = y\n",
    "    return self.function(yhat,y)\n",
    "  \n",
    "  def backward(self):\n",
    "    if not self.requires_grad:\n",
    "      raise RuntimeError('requires_grad set to False. Cannot calculate gradients.')\n",
    "    if not hasattr(self,'derivative_function'):\n",
    "      raise NameError('derivative_function not set for this loss function. Cannot calculate gradients')\n",
    "    return np.array([self.derivative_function(pred,gtruth) for pred,gtruth in zip(self.yhat, self.y)]).reshape(-1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1fbae8e-e58e-48bd-9b6a-31cfd0d68380",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "  \n",
    "\n",
    "class MSELoss(LossFunction):\n",
    "  def __init__(self, requires_grad=True):\n",
    "    super(MSELoss, self).__init__(requires_grad)\n",
    "    self.function = lambda yhat,y: np.square(yhat-y)\n",
    "    self.derivative_function = lambda yhat,y: (yhat-y)\n",
    "\n",
    "  def __call__(self, yhat,y):\n",
    "    super().__call__(yhat,y)\n",
    "    result = np.array([self.function(p,g) for p,g in zip(yhat,y)])\n",
    "    result = result.sum()/len(result)\n",
    "    return result\n",
    "    \n",
    "lossFn = MSELoss()\n",
    "lossFn(yhat=np.array([0,2]), y=np.array([3,4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "683d8f62-b9ad-496f-8ec5-a05e704a0f65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.303658313630144"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array([0.01, 0.99])\n",
    "lossFn = MSELoss()\n",
    "lossFn(ypred,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87488904-5e98-4367-ba73-c5346182d9a9",
   "metadata": {},
   "source": [
    "# backpropagation gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "986cdcfd-5487-4ba3-8c03-171f2f36f9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.backward(lossFn.backward())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ca485ca-1a27-4591-bb72-7c1639813f9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hidden1': {'weights': array([[0.00044758, 0.00089517],\n",
       "         [0.00056464, 0.00112929]]),\n",
       "  'bias': array([[0.00895169, 0.01129289]]),\n",
       "  'requires_grad': True},\n",
       " 'output': {'weights': array([[ 0.08169586,  0.08194416],\n",
       "         [-0.02356439, -0.02363601]]),\n",
       "  'bias': array([[ 0.13742501, -0.03963893]]),\n",
       "  'requires_grad': True}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_grads()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8faa1a7f-761a-4ae1-a43d-1c8ac54776e5",
   "metadata": {},
   "source": [
    "# Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9cfb2064-6c7d-4293-910a-2cde61005d54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    SGD ( \n",
       "      lr:0.01\n",
       "    )"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SGD():\n",
    "  def __init__(self, model,lr):\n",
    "    if not isinstance(model,NeuralNetwork):\n",
    "      raise TypeError(\"Input type must be of type NeuralNetwork.\")\n",
    "    self.model = model\n",
    "    self.lr = lr\n",
    "    pass\n",
    "  \n",
    "  def step(self):\n",
    "    for name, layer in self.model.param_references():\n",
    "      if not isinstance(layer, ActivationFunction):\n",
    "        if layer.requires_grad:\n",
    "          layer.weights += - self.lr*layer.get_grads()['weights']\n",
    "          if isinstance(layer.bias, np.ndarray):\n",
    "            layer.bias += - self.lr*layer.get_grads()['bias'].flatten()\n",
    "  def __repr__(self):\n",
    "    s = f'\\\n",
    "    SGD ( \\n\\\n",
    "      lr:{self.lr}\\n\\\n",
    "    )'\n",
    "    return s\n",
    "\n",
    "optimizer = SGD(model = model,lr=0.01)\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7aa72f3-c3d9-4bcf-b177-f4f104c1ed5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hidden1': {'weights': array([[0.15, 0.25],\n",
       "         [0.2 , 0.3 ]]),\n",
       "  'bias': array([0.35, 0.35])},\n",
       " 'output': {'weights': array([[0.4 , 0.5 ],\n",
       "         [0.45, 0.55]]),\n",
       "  'bias': array([0.6, 0.6])}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5895ca2-f47a-477d-aa26-139691f862fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hidden1': {'weights': array([[0.00044758, 0.00089517],\n",
       "         [0.00056464, 0.00112929]]),\n",
       "  'bias': array([[0.00895169, 0.01129289]]),\n",
       "  'requires_grad': True},\n",
       " 'output': {'weights': array([[ 0.08169586,  0.08194416],\n",
       "         [-0.02356439, -0.02363601]]),\n",
       "  'bias': array([[ 0.13742501, -0.03963893]]),\n",
       "  'requires_grad': True}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_grads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89a9196a-204d-4e31-a739-c9d4ef635469",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "380bfc2a-5209-42da-8d8b-7e45a130efdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hidden1': {'weights': array([[0.14999552, 0.24999105],\n",
       "         [0.19999435, 0.29998871]]),\n",
       "  'bias': array([0.34991048, 0.34988707])},\n",
       " 'output': {'weights': array([[0.39918304, 0.49918056],\n",
       "         [0.45023564, 0.55023636]]),\n",
       "  'bias': array([0.59862575, 0.60039639])}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
